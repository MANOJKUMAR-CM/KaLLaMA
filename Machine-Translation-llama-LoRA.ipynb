{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d7c82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb84b94c-a476-42bc-afc6-47ff66d78799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d2566b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import GenerationConfig, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import numpy as np\n",
    "import evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc808b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"ai4bharat/samanantar\", \"kn\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd12a381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling a smaller subset\n",
    "ds = ds['train'].train_test_split(5000, shuffle=True, seed=42)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79eab35",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ds['train'].shuffle(seed=42).select(range(25000))\n",
    "test_ds = ds['test']\n",
    "print(f\"Test Dataset: {test_ds}\")\n",
    "print(f\"Train Dataset: {train_ds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f5ad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds[0] # printing a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78b4164",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, model_max_length=512)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c02aa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_text(src, trgt=None):\n",
    "    if trgt is None:\n",
    "        return f\"\"\" Translate English to Kannada: English: {src}, Kannada:\"\"\"\n",
    "    else:\n",
    "        return f\"\"\" Translate English to Kannada: English: {src}, Kannada:{trgt}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529285b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(example):\n",
    "    src = example[\"src\"]\n",
    "    trgt = example[\"tgt\"]\n",
    "\n",
    "    full_text = format_text(src, trgt)\n",
    "\n",
    "    prompt_text = format_text(src)\n",
    "\n",
    "    # Tokenize full text\n",
    "    tokenized_full = tokenizer(\n",
    "        full_text,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        padding=False\n",
    "    )\n",
    "\n",
    "    # Tokenize prompt-only\n",
    "    tokenized_prompt = tokenizer(\n",
    "        prompt_text,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        padding=False\n",
    "    )\n",
    "\n",
    "    input_ids = tokenized_full[\"input_ids\"]\n",
    "\n",
    "    # Create labels\n",
    "    labels = input_ids.copy()\n",
    "\n",
    "    # Mask English + instruction tokens\n",
    "    prompt_len = len(tokenized_prompt[\"input_ids\"])\n",
    "    labels[:prompt_len] = [-100] * prompt_len\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": tokenized_full[\"attention_mask\"],\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7a406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized_ds = train_ds.map(tokenize_text, remove_columns=ds.column_names['train'])\n",
    "train_tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b64ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokenized_ds = test_ds.map(tokenize_text, remove_columns=ds.column_names['train'])\n",
    "test_tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4283a018",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c800b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.pad_token = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf7e933",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384d77f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model = get_peft_model(model, lora_config)\n",
    "lora_model.print_trainable_parameters()\n",
    "lora_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54eee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=lora_model,\n",
    "    padding=True,          # REQUIRED\n",
    "    label_pad_token_id=-100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35837169",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = train_tokenized_ds[0]\n",
    "outputs = lora_model(\n",
    "    input_ids=torch.tensor([batch[\"input_ids\"]]).cuda(),\n",
    "    labels=torch.tensor([batch[\"labels\"]]).cuda()\n",
    ")\n",
    "\n",
    "print(outputs.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfe10d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    \n",
    "    # In case the model returns more than just logits\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    # Decode predictions and labels\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    preds = np.where(preds < 0, tokenizer.pad_token_id, preds)\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Post-processing: extract only the Kannada part if needed\n",
    "    # (Note: During training, the labels only contain the Kannada part because we masked the prompt)\n",
    "    decoded_preds = [pred.split(\"Kannada:\")[-1].strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    \n",
    "    return {\"bleu\": result[\"score\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874bc86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.generation_config = GenerationConfig(\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False,   # important for eval stability\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6e3db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./llama-kannada-lora\",\n",
    "    eval_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    \n",
    "    eval_steps=250,          # Evaluate every 200 steps\n",
    "    save_steps=500,\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True, # CRITICAL: This enables the generation loop\n",
    "\n",
    "    fp16=True,                  # Faster training on most GPUs\n",
    "    \n",
    "    logging_steps=100,\n",
    "    num_train_epochs=1,\n",
    "    report_to=\"none\",\n",
    "\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized_ds,\n",
    "    eval_dataset=test_tokenized_ds.select(range(100)), # Sample for faster eval\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c7ec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training!\n",
    "result = trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
